{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landfill mining with reinforcement learning and Mobile robotics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual Study, Joseph Maliszewski "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A robot will need to navigate its way through enviroments at landfill. To do this it will need to avoid obstacles. We can do the following:\n",
    "<br>\n",
    "<br>\n",
    "1) A human to add rules to the system in order to genrate desired behaviour \n",
    "<br>\n",
    "<br>\n",
    "2) Use self-learning such as RL, for the system to do this by iteself. \n",
    "<br>\n",
    "<br>\n",
    "3) A hybrid of the two\n",
    "<br>\n",
    "<br>\n",
    "As the system will need to be a scalable system in this enviroment. It may be better to use a RL approach, (or a hybrid of the two). This will remove the need for any large edge cases to be accounted for, thus lowering need for human maintainance or intevention in development, are large likehood for optimal behaviour to be reached increasing the effciency of the system.\n",
    "<br>\n",
    "<br>\n",
    "This notebook takes a very simple game from the openAI gym library, to present simple example of how RL could be implemented on for a single agent mining useful resources from landfill, using a markov descion process. In future, as described in the report, multiple nested, multi-agent reinforment learning would then be the next step for implementing a multi-robot colabrative system on a landfill. \n",
    "<br>\n",
    "<br>\n",
    "Reinforcement learning is a machine learning technique, that learns to act/behave in a partcilar way to maximze a given reward. Markov descion proccesses (MDP) is a discrete time stochastic control process allows for formalization of sequential descion making, such as this. In this example, MDP describes the problem, where RF can be used to find solutions to this problesm. in other words we are looking to discover an MDP model that provides a solution to the problem, with RL. There are 5 components to MDP, \n",
    "Enviroment, agent, states, Actions, Rewards. These work seqentially, where a reward is given depending on a action in a given state. This is called a trajectory.\n",
    " <br>   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Landfill Game "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robot in starting position is at a 2mx2m container on the edge of landfill in simulation.\n",
    "<br>\n",
    "<br>\n",
    "The task for robot is to fill the container with clear plastic bottles collected from the landfill. \n",
    "<br>\n",
    "<br>\n",
    "The robot has been notified of a GPS location within an area at the landfill that is newly discovered and that it is there are clear plastic bottles in abundance. This was communicated by another robot that has emmited an \"artifical pheromone\" indicting the present of this resource to other robots in the area.\n",
    "<br>\n",
    "<br>\n",
    "The robot must learn to avoid all obstacles on its way to this location of the landfill with no prior knowledge of its environment.\n",
    "<br>\n",
    "<br>\n",
    "The landfill site can be represented as a grid:\n",
    "<br>\n",
    "<br>\n",
    "SFFH<br>\n",
    "FHFF<br>\n",
    "HFFH<br>\n",
    "FHFG<br>\n",
    "<br>\n",
    "S = robot (agent)starting position <br>\n",
    "F = Free space in landfill/viable path<br>\n",
    "H = Obstacle in landfill <br>\n",
    "G = The goal destination of abundent clear plastics bottles<br>\n",
    "<br>\n",
    "The robot can navigate up, down, left, right, where the episode is terminated at the goal or if the robot hits the obstacle. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, ' : ', '0.01900000000000001')\n",
      "(2000, ' : ', '0.025000000000000015')\n",
      "(3000, ' : ', '0.06300000000000004')\n",
      "(4000, ' : ', '0.057000000000000044')\n",
      "(5000, ' : ', '0.11400000000000009')\n",
      "(6000, ' : ', '0.09500000000000007')\n",
      "(7000, ' : ', '0.10700000000000008')\n",
      "(8000, ' : ', '0.11900000000000009')\n",
      "(9000, ' : ', '0.10400000000000008')\n",
      "(10000, ' : ', '0.09400000000000007')\n",
      "[[2.03807538e-17 4.06286863e-18 2.74376063e-27 8.95868595e-28]\n",
      " [1.16374460e-41 4.42116666e-29 3.21175758e-37 3.72905871e-12]\n",
      " [7.53344398e-13 6.33982839e-26 2.41193366e-35 3.74954344e-26]\n",
      " [1.34440906e-22 3.02702778e-37 1.31847935e-35 2.70073483e-36]\n",
      " [2.20741735e-17 3.18003922e-35 2.81602050e-24 5.98833426e-31]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.30563943e-25 3.80476567e-14 1.09902438e-26 1.04004085e-34]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.15220215e-31 9.54536995e-13 1.90461159e-20 2.52003891e-31]\n",
      " [9.60608665e-13 1.93118980e-08 2.07655573e-13 6.91162946e-25]\n",
      " [7.60991565e-09 2.33214366e-22 1.95030433e-09 5.85738098e-15]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [9.08644467e-28 3.25481322e-25 1.83303290e-23 6.10384009e-13]\n",
      " [1.23010646e-15 9.90001970e-01 6.26637695e-16 6.82136536e-16]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "def build_empty_qtable(num_poss_actions, total_num_states):\n",
    "    #qtable hold all pairs of states and possible action can can be taken in that state\n",
    "    #actions: Left,Right,Up,Down : States: each unit of area in the enviroment\n",
    "    # This table continously updates are referenced to and is the basis for how the agent learns\n",
    "    # from its envirmoment.\n",
    "    \n",
    "    return np.zeros((num_poss_actions,total_num_states))\n",
    "\n",
    "def update_qtable(qtable, learn_r, reward, discount_r, current_state,new_state, action):\n",
    "    \n",
    "    #The new state is a weighted sum of the new value and the old value. \n",
    "    \n",
    "    old_value = qtable[current_state, action]\n",
    "    new_value = reward + discount_r*(np.max(qtable[new_state, :]))\n",
    "    qtable[current_state, action] =  ((1-learn_r) * old_value) + (learn_r*new_value)\n",
    "    return qtable\n",
    "\n",
    "\n",
    "def decay_exploration_rate(explore_r, min_explore_r, max_explore_r, explore_decay_r, episode):\n",
    "    explore_r = min_explore_r + (max_explore_r - min_explore_r)*np.exp(-explore_decay_r*episode)\n",
    "    return explore_r\n",
    "    \n",
    "\n",
    "def main():\n",
    "    \n",
    "    #Get the landfill enviroment (modelled of provided openAI frozen lake)\n",
    "    environment = gym.make(\"FrozenLake-v0\")\n",
    "    \n",
    "    #to build qtable need to num of poss actions in a state, and the number of total states\n",
    "    #Qtable is a combination of every possible state and action pair. \n",
    "    num_poss_actions = environment.action_space.n\n",
    "    total_num_states = environment.observation_space.n\n",
    "    qtable = build_empty_qtable(total_num_states, num_poss_actions)\n",
    "    \n",
    "    #Set params for how long we want it to learn for \n",
    "    num_episodes = 10000\n",
    "    num_steps = 100\n",
    "    \n",
    "    #set exploration/exploitation trade off params\n",
    "    explore_r = 1\n",
    "    explore_decay_r = 0.001\n",
    "    max_explore_r = 1\n",
    "    min_explore_r = 0.01\n",
    "    \n",
    "    discount_r = 0.1\n",
    "    learn_r = 0.99\n",
    "    \n",
    "    # obtain all the rewards from all episodes to see learning progress \n",
    "    all_rewards_all_episodes = []\n",
    "  \n",
    "    # Start the learing process\n",
    "    for episode in range(num_episodes):\n",
    "      \n",
    "        #one the start of each episode, the enviroment must be reset. \n",
    "        current_state = environment.reset()\n",
    "        #print(current_state)\n",
    "        \n",
    "        #Will tell us when the episode is finished,so needs to be reset at the begging of each episode\n",
    "        episode_complete = False\n",
    "        \n",
    "        #total rewards for current episode reset\n",
    "        rewards_current_episode = 0\n",
    "        \n",
    "        #for all time steps in each episode (t in T)\n",
    "        for steps in range(num_steps):\n",
    "                                 \n",
    "    \n",
    "            #exploration/explotation trade off. If over limit then exploitation will occur\n",
    "            #if under limit, exploration will occur. \n",
    "        \n",
    "            explore_limit = random.uniform(0,1)\n",
    "            \n",
    "            \n",
    "           # print(\"explore_limit \", explore_limit, \"explore_r \", explore_r)\n",
    "            \n",
    "            \n",
    "            if(explore_limit > explore_r):\n",
    "\n",
    "                #exploit - does this by taking the maximum action value in the current state found to date.\n",
    "                #If the learning rate starts at 1, it will always explore to begin with, but over time, it will be less\n",
    "                #likely to explore, an would be more likely to exploit.\n",
    "                action = np.argmax(qtable[current_state, : ])\n",
    "        \n",
    "            else:\n",
    "                #explore - takes a random action in that current state.\n",
    "                action = environment.action_space.sample() \n",
    "              \n",
    "            \n",
    "            \n",
    "            #Now the action has been decided, the action must be excecuted. \n",
    "            #It will tell us :\n",
    "                #- what the new state the action has taken the agent to\n",
    "                #- the reward recieved from the action taken\n",
    "                #- whether or not the agent has reached a terminal state (eg, collsion or goal)\n",
    "                #- info diagnostics, regarding enviroment\n",
    "            new_state, reward, episode_complete, info= environment.step(action)\n",
    "            \n",
    "            \n",
    "            rewards_current_episode = rewards_current_episode + reward                    \n",
    "            #print(\"rewards_current_episode  \",rewards_current_episode)\n",
    "            #we can now update the qtable with the information of the action and state. It does not replace\n",
    "            #the old value but combines wthis with the new learnt value. (relative to learning rate). Over time,\n",
    "            #new learnt values have lesser effect on the qtable value, and the old value is less adjustable.\n",
    "            qtable = update_qtable(qtable, learn_r, reward, discount_r, current_state, new_state, action)\n",
    "                                     \n",
    "           # print(qtable)\n",
    "            \n",
    "            current_state = new_state\n",
    "                                \n",
    "            #print(current_state)\n",
    "            #checks to see if agent has reached a terminal state, if so, it breaks out of time steps                             \n",
    "            if (episode_complete == True):\n",
    "                break\n",
    "                                 \n",
    "        explore_r = decay_exploration_rate(explore_r, min_explore_r, max_explore_r, explore_decay_r, episode)                       \n",
    "        \n",
    "        all_rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "    rewards_per_1000_episodes = np.split(np.array(all_rewards_all_episodes), num_episodes/1000)\n",
    "    #print(\"rewards_per_1000_episodes \", len(rewards_per_1000_episodes))\n",
    "    count = 1000\n",
    "    \n",
    "    rewards_1000_eps = []\n",
    "    for r in rewards_per_1000_episodes:\n",
    "        rewards_1000_eps.append(str(sum(r/1000)))\n",
    "        print(count, \" : \", str(sum(r/1000)))\n",
    "        count += 1000\n",
    "    print(qtable)\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. date accessed 19 march 2020 https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da\n",
    "2. date accessed 19 march 2020 https://link.springer.com/chapter/10.1007/978-3-642-27645-3_1\n",
    "3. date accessed 19 march 2020 http://karpathy.github.io/2016/05/31/rl/\n",
    "4. date accessed 20 march 2020 https://towardsdatascience.com/reinforcement-learning-rl-101-with-python-e1aa0d37d43b\n",
    "5. date accessed 20 march 2020 https://deeplizard.com/learn/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
